{
  "metadata": {
    "test_type": "latency_check",
    "version": "1.0.0",
    "description": "RAG system latency benchmarks for attention paper queries with realistic performance expectations"
  },
  "latency_benchmarks": [
    {
      "benchmark_type": "simple_rag_queries",
      "description": "Basic questions about attention mechanisms",
      "max_latency_ms": 2000,
      "test_prompts": [
        {
          "prompt": "What is attention?",
          "expected_max_latency_ms": 1500,
          "complexity": "simple",
          "token_estimate": 50,
          "rag_operations": ["simple_retrieval", "basic_generation"]
        },
        {
          "prompt": "Define self-attention",
          "expected_max_latency_ms": 1800,
          "complexity": "simple",
          "token_estimate": 75,
          "rag_operations": ["concept_retrieval", "definition_generation"]
        },
        {
          "prompt": "What are attention heads?",
          "expected_max_latency_ms": 1600,
          "complexity": "simple",
          "token_estimate": 60,
          "rag_operations": ["terminology_lookup", "explanation_generation"]
        },
        {
          "prompt": "Who wrote the attention paper?",
          "expected_max_latency_ms": 1200,
          "complexity": "trivial",
          "token_estimate": 30,
          "rag_operations": ["metadata_retrieval", "factual_response"]
        }
      ]
    },
    {
      "benchmark_type": "medium_rag_queries",
      "description": "Moderate complexity questions requiring reasoning across multiple paper sections",
      "max_latency_ms": 4000,
      "test_prompts": [
        {
          "prompt": "How does multi-head attention work and why is it better than single-head?",
          "expected_max_latency_ms": 3500,
          "complexity": "medium",
          "token_estimate": 200,
          "rag_operations": ["multi_section_retrieval", "comparative_analysis", "detailed_generation"]
        },
        {
          "prompt": "Compare encoder and decoder attention mechanisms in transformers",
          "expected_max_latency_ms": 3800,
          "complexity": "medium",
          "token_estimate": 250,
          "rag_operations": ["comparative_retrieval", "structural_analysis", "comparison_generation"]
        },
        {
          "prompt": "Explain the mathematical formulation of scaled dot-product attention",
          "expected_max_latency_ms": 3200,
          "complexity": "medium",
          "token_estimate": 180,
          "rag_operations": ["formula_retrieval", "mathematical_explanation", "technical_generation"]
        },
        {
          "prompt": "What are the advantages of transformers over RNNs and CNNs?",
          "expected_max_latency_ms": 3600,
          "complexity": "medium",
          "token_estimate": 220,
          "rag_operations": ["advantage_analysis", "comparative_retrieval", "structured_response"]
        }
      ]
    },
    {
      "benchmark_type": "complex_rag_queries",
      "description": "Complex questions requiring deep analysis and synthesis across the entire paper",
      "max_latency_ms": 6000,
      "test_prompts": [
        {
          "prompt": "Provide a comprehensive analysis of how positional encoding works in transformers and why it's necessary",
          "expected_max_latency_ms": 5500,
          "complexity": "high",
          "token_estimate": 400,
          "rag_operations": ["comprehensive_retrieval", "deep_analysis", "detailed_synthesis", "long_generation"]
        },
        {
          "prompt": "Explain the complete transformer architecture including all components and their interactions",
          "expected_max_latency_ms": 5800,
          "complexity": "high",
          "token_estimate": 500,
          "rag_operations": ["full_architecture_retrieval", "component_analysis", "interaction_mapping", "comprehensive_generation"]
        },
        {
          "prompt": "Analyze the experimental results and compare transformer performance with baseline models across different tasks",
          "expected_max_latency_ms": 5200,
          "complexity": "high",
          "token_estimate": 350,
          "rag_operations": ["results_retrieval", "performance_analysis", "cross_task_comparison", "analytical_generation"]
        },
        {
          "prompt": "Discuss the implications of the attention mechanism for future NLP research and potential improvements",
          "expected_max_latency_ms": 6000,
          "complexity": "high",
          "token_estimate": 450,
          "rag_operations": ["implication_analysis", "future_research_synthesis", "improvement_discussion", "forward_looking_generation"]
        }
      ]
    },
    {
      "benchmark_type": "edge_case_queries",
      "description": "Edge cases that might cause performance issues",
      "max_latency_ms": 8000,
      "test_prompts": [
        {
          "prompt": "What does the paper say about " + "attention " * 50 + "mechanisms?",
          "expected_max_latency_ms": 7000,
          "complexity": "edge_case",
          "token_estimate": 600,
          "rag_operations": ["repetitive_query_handling", "noise_filtering", "relevant_extraction"]
        },
        {
          "prompt": "Explain every single mathematical equation in the attention paper in complete detail",
          "expected_max_latency_ms": 8000,
          "complexity": "edge_case",
          "token_estimate": 800,
          "rag_operations": ["exhaustive_retrieval", "mathematical_compilation", "comprehensive_explanation"]
        }
      ]
    }
  ],
  "performance_thresholds": {
    "p50_latency_ms": 2500,
    "p95_latency_ms": 5000,
    "p99_latency_ms": 7000,
    "timeout_ms": 15000,
    "error_rate_threshold": 0.02,
    "rag_specific_metrics": {
      "retrieval_latency_ms": 500,
      "embedding_latency_ms": 200,
      "generation_latency_ms": 2000,
      "total_pipeline_overhead_ms": 300
    }
  },
  "rag_performance_benchmarks": [
    {
      "benchmark_type": "retrieval_speed",
      "description": "Vector similarity search performance",
      "max_latency_ms": 800,
      "test_prompts": [
        {
          "prompt": "attention mechanism",
          "expected_max_latency_ms": 300,
          "complexity": "simple_retrieval",
          "token_estimate": 10,
          "rag_operations": ["embedding_generation", "vector_search"]
        },
        {
          "prompt": "multi-head attention mathematical formulation with positional encoding",
          "expected_max_latency_ms": 600,
          "complexity": "complex_retrieval",
          "token_estimate": 50,
          "rag_operations": ["complex_embedding", "multi_concept_search"]
        }
      ]
    },
    {
      "benchmark_type": "generation_speed",
      "description": "Response generation performance with retrieved context",
      "max_latency_ms": 3000,
      "test_prompts": [
        {
          "prompt": "Summarize the key findings of the attention paper",
          "expected_max_latency_ms": 2800,
          "complexity": "summary_generation",
          "token_estimate": 300,
          "rag_operations": ["context_synthesis", "structured_generation"]
        }
      ]
    }
  ],
  "stress_test_scenarios": [
    {
      "scenario_name": "concurrent_queries",
      "description": "Multiple simultaneous queries to test system under load",
      "concurrent_requests": 10,
      "query": "What is the transformer architecture?",
      "expected_max_latency_ms": 4000,
      "expected_success_rate": 0.95
    },
    {
      "scenario_name": "rapid_sequential",
      "description": "Rapid sequential queries to test caching and optimization",
      "requests_per_second": 5,
      "duration_seconds": 30,
      "query_pattern": ["What is attention?", "How does self-attention work?", "What are transformers?"],
      "expected_avg_latency_ms": 2000
    }
  ]
}
