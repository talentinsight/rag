{"qid": "att_hard_q1", "question": "Compare the computational complexity of self-attention versus recurrent layers", "answer": "Self-attention has O(n²·d) complexity per layer while recurrent layers have O(n·d²) complexity, where n is sequence length and d is representation dimension.", "contexts": ["att_p3", "att_p1"], "meta": {"category": "complexity", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["What is the computational difference between attention and RNNs?", "Compare attention and recurrent complexity"], "require_hybrid": true, "mmr_lambda": 0.2}}
{"qid": "att_hard_q2", "question": "Why does the paper divide by √dk in the attention formula?", "answer": "Dividing by √dk prevents the dot products from growing large in magnitude, which would push the softmax function into regions with extremely small gradients.", "contexts": ["att_p8", "att_p7"], "meta": {"category": "scaled_dot_product", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["cot", "scaffold"], "paraphrase_runs": 2}}
{"qid": "att_hard_q3", "question": "What is the relationship between dmodel, h, and dk in multi-head attention?", "answer": "In multi-head attention, dk = dv = dmodel/h, where h is the number of heads. With h=8 and dmodel=512, each head has dk=dv=64.", "contexts": ["att_p15", "att_p12"], "meta": {"category": "multi_head", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q4", "question": "How does the masking mechanism preserve auto-regressive properties?", "answer": "Masking sets illegal connections to negative infinity in the softmax input, ensuring predictions for position i depend only on known outputs at positions less than i.", "contexts": ["att_p28", "att_p20"], "meta": {"category": "masking", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Explain how masking maintains causality", "Why is masking important for generation?"], "require_hybrid": true}}
{"qid": "att_hard_q5", "question": "What is the mathematical intuition behind using sine and cosine for positional encoding?", "answer": "Sine and cosine functions allow the model to learn relative positions because PE_pos+k can be represented as a linear function of PE_pos for any fixed offset k.", "contexts": ["att_p24", "att_p23"], "meta": {"category": "positional_encoding", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q6", "question": "How do the three types of attention in Transformer differ in their key-value-query sources?", "answer": "Encoder self-attention: Q,K,V from same encoder layer. Decoder self-attention: Q,K,V from same decoder layer (masked). Encoder-decoder attention: Q from decoder, K,V from encoder.", "contexts": ["att_p25", "att_p27", "att_p29"], "meta": {"category": "attention_types", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Compare the three attention mechanisms", "What are the differences between attention types?"], "synonyms": ["mechanisms", "types", "variants"]}}
{"qid": "att_hard_q7", "question": "Why does multi-head attention concatenate heads before the output projection?", "answer": "Concatenating heads combines information from different representation subspaces, then the output projection W^O integrates this multi-perspective information into the final representation.", "contexts": ["att_p13", "att_p11"], "meta": {"category": "multi_head", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q8", "question": "What role does the feed-forward network play in each Transformer layer?", "answer": "The position-wise fully connected feed-forward network processes each position independently, applying the same learned transformation to each position in the sequence.", "contexts": ["att_p17"], "meta": {"category": "architecture", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["simple", "cot"], "paraphrase_runs": 1}}
{"qid": "att_hard_q9", "question": "How does layer normalization interact with residual connections in the Transformer?", "answer": "Layer normalization is applied after adding the residual connection: LayerNorm(x + Sublayer(x)), where x is the input and Sublayer(x) is the sub-layer output.", "contexts": ["att_p18"], "meta": {"category": "architecture", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q10", "question": "What is the significance of the attention weights being a probability distribution?", "answer": "The softmax function ensures attention weights sum to 1, creating a weighted average of values where the model can focus more on relevant positions while still considering all positions.", "contexts": ["att_p8", "att_p6"], "meta": {"category": "attention_mechanism", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Why use softmax for attention weights?", "What does softmax do in attention?"], "require_hybrid": true, "mmr_lambda": 0.4}}
{"qid": "att_hard_q11", "question": "How does the Transformer handle variable sequence lengths during training and inference?", "answer": "The paper mentions using padding and masking techniques, though specific implementation details for variable lengths are not fully detailed in the provided passages.", "contexts": ["att_p28"], "meta": {"category": "implementation", "difficulty": "hard"}, "task_type": "rag_qa", "required": false}
{"qid": "att_hard_q12", "question": "What is the computational advantage of parallel attention computation?", "answer": "Computing attention on a matrix of queries simultaneously (packed into Q, K, V matrices) allows for efficient parallel computation compared to sequential processing.", "contexts": ["att_p9"], "meta": {"category": "efficiency", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q13", "question": "Why might different attention heads learn different behaviors?", "answer": "Different attention heads have separate learned projection matrices (W^Q_i, W^K_i, W^V_i), allowing them to focus on different aspects like syntactic and semantic structure.", "contexts": ["att_p32", "att_p14"], "meta": {"category": "interpretability", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["cot", "scaffold"], "paraphrase_runs": 2}}
{"qid": "att_hard_q14", "question": "What is the relationship between attention and translation alignment?", "answer": "Encoder-decoder attention allows every decoder position to attend over all input positions, mimicking traditional alignment mechanisms in sequence-to-sequence models.", "contexts": ["att_p30"], "meta": {"category": "translation", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_hard_q15", "question": "How does the Transformer's architecture enable better parallelization than RNNs?", "answer": "By eschewing recurrence entirely and relying on attention mechanisms, the Transformer removes sequential dependencies that prevent parallel computation in RNNs.", "contexts": ["att_p1", "att_p2"], "meta": {"category": "parallelization", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Why is Transformer more parallelizable than RNNs?", "How does attention enable parallelization?"], "require_hybrid": true}}
