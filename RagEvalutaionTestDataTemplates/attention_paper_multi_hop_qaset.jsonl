{"qid": "att_multi_q1", "question": "How do the encoder and decoder work together through attention to perform translation?", "answer": "The encoder processes the input sequence using self-attention to create representations, then the decoder uses encoder-decoder attention to access these representations while generating the output sequence using masked self-attention.", "contexts": ["att_p17", "att_p19", "att_p29", "att_p30"], "meta": {"category": "architecture", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Explain encoder-decoder interaction in Transformer", "How does Transformer perform translation?"], "require_hybrid": true, "mmr_lambda": 0.3}}
{"qid": "att_multi_q2", "question": "Trace the complete flow of information from input embeddings to final output in the Transformer", "answer": "Input embeddings are added to positional encodings, then processed through 6 encoder layers with self-attention and feed-forward networks. The decoder takes output embeddings with positional encoding, uses masked self-attention, encoder-decoder attention to access encoder outputs, and feed-forward networks to generate the final output.", "contexts": ["att_p22", "att_p17", "att_p19", "att_p25", "att_p29"], "meta": {"category": "architecture", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q3", "question": "How does the multi-head attention mechanism combine different types of attention to create the final representation?", "answer": "Multi-head attention projects queries, keys, and values through different learned projections for each head, computes attention for each head separately, concatenates the results, and applies a final output projection to combine information from different representation subspaces.", "contexts": ["att_p12", "att_p13", "att_p14", "att_p11"], "meta": {"category": "multi_head", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["cot", "scaffold"], "paraphrase_runs": 2}}
{"qid": "att_multi_q4", "question": "Why does the Transformer need both positional encoding and attention mechanisms to handle sequences?", "answer": "Since the Transformer has no recurrence or convolution, it needs positional encodings to inject sequence order information. Attention mechanisms then allow modeling dependencies between any positions regardless of distance, combining positional awareness with content-based interactions.", "contexts": ["att_p21", "att_p22", "att_p3", "att_p5"], "meta": {"category": "sequence_modeling", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Why are both position and attention needed?", "How do position and attention work together?"], "require_hybrid": true}}
{"qid": "att_multi_q5", "question": "How does the training setup with different datasets contribute to the model's translation performance?", "answer": "Training on WMT 2014 English-German (4.5M pairs) and English-French (36M pairs) datasets with byte-pair encoding and shared vocabularies, combined with the Transformer's attention-based architecture, achieved state-of-the-art BLEU scores of 28.4 and 41.0 respectively.", "contexts": ["att_p34", "att_p35", "att_p36", "att_p37", "att_p38"], "meta": {"category": "training_results", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q6", "question": "How do the different attention mechanisms in the decoder prevent information leakage while enabling translation?", "answer": "Decoder self-attention uses masking to prevent attending to future positions, preserving causality. Encoder-decoder attention allows accessing all encoder positions for translation context. This combination enables auto-regressive generation while utilizing full source information.", "contexts": ["att_p27", "att_p28", "att_p29", "att_p30"], "meta": {"category": "decoder_attention", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["How does decoder attention work for translation?", "Explain decoder attention mechanisms"], "synonyms": ["mechanisms", "processes", "operations"]}}
{"qid": "att_multi_q7", "question": "What is the relationship between model size, training time, and performance in the Transformer experiments?", "answer": "The base model trained in 3.5 days while the big model (with larger dimensions: dmodel=1024, h=16, dff=4096) required 12 days on 8 P100 GPUs, suggesting that larger models need significantly more training time but may achieve better performance.", "contexts": ["att_p39", "att_p40"], "meta": {"category": "scaling", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q8", "question": "How does the Transformer's attention mechanism enable better interpretability compared to recurrent models?", "answer": "Self-attention creates interpretable attention distributions where individual heads learn different tasks related to syntactic and semantic structure, unlike RNNs where information is compressed into hidden states that are harder to interpret.", "contexts": ["att_p31", "att_p32", "att_p1"], "meta": {"category": "interpretability", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["simple", "cot"], "paraphrase_runs": 1}}
{"qid": "att_multi_q9", "question": "How do residual connections and layer normalization work together with attention to enable deep networks?", "answer": "Residual connections allow gradients to flow directly through the network via x + Sublayer(x), while layer normalization stabilizes training. This combination with attention mechanisms enables stacking 6 layers in both encoder and decoder.", "contexts": ["att_p18", "att_p17", "att_p19"], "meta": {"category": "deep_architecture", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q10", "question": "Why is the choice of sine and cosine functions for positional encoding particularly suitable for the attention mechanism?", "answer": "Sine and cosine functions allow relative position learning because PE_pos+k can be expressed as a linear function of PE_pos. This property enables attention mechanisms to easily learn to attend by relative positions, which is crucial for sequence understanding.", "contexts": ["att_p23", "att_p24", "att_p21"], "meta": {"category": "positional_encoding", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Why sine/cosine for positions with attention?", "How do trigonometric functions help attention?"], "require_hybrid": true, "mmr_lambda": 0.4}}
{"qid": "att_multi_q11", "question": "How does the Transformer's architecture address the limitations of both RNNs and CNNs for sequence modeling?", "answer": "The Transformer eschews recurrence (avoiding RNN's sequential processing bottleneck) and convolutions (avoiding CNN's limited receptive field) by using attention mechanisms that can model dependencies at any distance with parallel computation.", "contexts": ["att_p1", "att_p2", "att_p3"], "meta": {"category": "architecture_comparison", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q12", "question": "What is the complete mathematical flow from input to attention weights in scaled dot-product attention?", "answer": "Input queries Q and keys K undergo dot product multiplication (QK^T), results are scaled by dividing by âˆšdk to prevent large magnitudes, then softmax is applied to create probability distributions that serve as attention weights for the values V.", "contexts": ["att_p8", "att_p10", "att_p7"], "meta": {"category": "attention_computation", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["cot", "scaffold"], "paraphrase_runs": 2}}
{"qid": "att_multi_q13", "question": "How do the experimental results demonstrate the effectiveness of the attention-only approach?", "answer": "The Transformer achieved state-of-the-art BLEU scores (28.4 English-German, 41.0 English-French) while being more parallelizable than RNN/CNN models, proving that attention mechanisms alone can effectively handle sequence-to-sequence tasks without recurrence or convolution.", "contexts": ["att_p37", "att_p38", "att_p1", "att_p2"], "meta": {"category": "experimental_validation", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_multi_q14", "question": "How does the multi-head attention design enable the model to capture different types of linguistic relationships?", "answer": "By using multiple attention heads with separate projection matrices, the model can simultaneously attend to different representation subspaces, allowing different heads to specialize in various linguistic phenomena like syntax, semantics, and positional relationships.", "contexts": ["att_p11", "att_p12", "att_p32", "att_p14"], "meta": {"category": "linguistic_modeling", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["How do multiple heads capture language patterns?", "Why use multiple attention heads for language?"], "require_hybrid": true}}
{"qid": "att_multi_q15", "question": "What is the complete process of how the Transformer generates a translation from start to finish?", "answer": "The encoder processes source tokens with positional encoding through self-attention layers. The decoder starts with a start token, uses masked self-attention for generated tokens, encoder-decoder attention to access source information, and generates tokens auto-regressively until an end token, with each step building on previous outputs while attending to the full source.", "contexts": ["att_p17", "att_p19", "att_p20", "att_p22", "att_p29", "att_p30"], "meta": {"category": "translation_process", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
