{"qid": "att_q1", "question": "What is the Transformer?", "answer": "The Transformer is a model architecture that relies entirely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "contexts": ["att_p1", "att_p2"], "meta": {"category": "architecture", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q2", "question": "What does an attention function do?", "answer": "An attention function maps a query and a set of key-value pairs to an output, where the output is computed as a weighted sum of the values.", "contexts": ["att_p5", "att_p6"], "meta": {"category": "attention_mechanism", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q3", "question": "What is the formula for Scaled Dot-Product Attention?", "answer": "Attention(Q, K, V) = softmax(QK^T / √dk)V", "contexts": ["att_p10"], "meta": {"category": "formula", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q4", "question": "Why is multi-head attention beneficial?", "answer": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.", "contexts": ["att_p11", "att_p12"], "meta": {"category": "multi_head", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["What are the benefits of multi-head attention?", "Why use multiple attention heads?"], "synonyms": ["multi-head", "multiple heads"], "require_hybrid": true, "mmr_lambda": 0.4}}
{"qid": "att_q5", "question": "What is the formula for Multi-Head Attention?", "answer": "MultiHead(Q, K, V) = Concat(head1, ..., headh)W^O where headi = Attention(QW^Q_i, KW^K_i, VW^V_i)", "contexts": ["att_p13"], "meta": {"category": "formula", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q6", "question": "How many attention heads does the Transformer use?", "answer": "The Transformer uses h = 8 parallel attention heads, with dk = dv = dmodel/h = 64 for each head.", "contexts": ["att_p15"], "meta": {"category": "multi_head", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q7", "question": "How many layers does the encoder have?", "answer": "The encoder is composed of a stack of N = 6 identical layers.", "contexts": ["att_p17"], "meta": {"category": "encoder", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q8", "question": "What are the two sub-layers in each encoder layer?", "answer": "Each encoder layer has a multi-head self-attention mechanism and a position-wise fully connected feed-forward network.", "contexts": ["att_p17"], "meta": {"category": "encoder", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["What components are in each encoder layer?", "Describe the encoder layer structure"], "synonyms": ["sub-layers", "components", "parts"]}}
{"qid": "att_q9", "question": "How does the decoder differ from the encoder?", "answer": "The decoder has the same two sub-layers as the encoder, plus a third sub-layer that performs multi-head attention over the encoder output.", "contexts": ["att_p19"], "meta": {"category": "decoder", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q10", "question": "What is masking in the decoder and why is it needed?", "answer": "Masking prevents positions from attending to subsequent positions by setting illegal connections to negative infinity, preserving the auto-regressive property.", "contexts": ["att_p20", "att_p28"], "meta": {"category": "masking", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Why does the decoder use masking?", "Explain decoder masking"], "require_hybrid": true}}
{"qid": "att_q11", "question": "Why does the Transformer need positional encoding?", "answer": "Since the model contains no recurrence and no convolution, positional encodings are needed to inject information about the position of tokens in the sequence.", "contexts": ["att_p21", "att_p22"], "meta": {"category": "positional_encoding", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q12", "question": "What functions are used for positional encoding?", "answer": "Sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/10000^(2i/dmodel)) and PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))", "contexts": ["att_p23"], "meta": {"category": "positional_encoding", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q13", "question": "How does self-attention work in the encoder?", "answer": "In encoder self-attention layers, all keys, values and queries come from the output of the previous layer, and each position can attend to all positions in the previous layer.", "contexts": ["att_p25", "att_p26"], "meta": {"category": "self_attention", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["simple", "cot"], "paraphrase_runs": 2}}
{"qid": "att_q14", "question": "How does encoder-decoder attention work?", "answer": "In encoder-decoder attention, queries come from the previous decoder layer, while keys and values come from the encoder output, allowing every decoder position to attend to all input positions.", "contexts": ["att_p29", "att_p30"], "meta": {"category": "encoder_decoder_attention", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q15", "question": "What makes attention mechanisms interpretable?", "answer": "Individual attention heads learn to perform different tasks and exhibit behavior related to syntactic and semantic structure, making the models more interpretable.", "contexts": ["att_p31", "att_p32"], "meta": {"category": "interpretability", "difficulty": "medium"}, "task_type": "rag_qa", "required": false}
{"qid": "att_q16", "question": "What datasets were used for training?", "answer": "WMT 2014 English-German dataset with 4.5 million sentence pairs and WMT 2014 English-French dataset with 36 million sentences.", "contexts": ["att_p34", "att_p35"], "meta": {"category": "training", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q17", "question": "What BLEU scores did the Transformer achieve?", "answer": "28.4 BLEU on English-German translation and 41.0 BLEU on English-French translation, both setting new state-of-the-art results.", "contexts": ["att_p37", "att_p38"], "meta": {"category": "results", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q18", "question": "How long did training take?", "answer": "Training took 3.5 days on 8 P100 GPUs for the base models and 12 days for the big models.", "contexts": ["att_p39"], "meta": {"category": "training", "difficulty": "easy"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q19", "question": "What are the parameters of the big Transformer model?", "answer": "The big model has N = 6 layers, dmodel = 1024, dff = 4096, h = 16 attention heads, dk = dv = 64, and dropout = 0.3.", "contexts": ["att_p40"], "meta": {"category": "model_details", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q20", "question": "How is the attention weight computed in scaled dot-product attention?", "answer": "The attention weights are computed by taking dot products of the query with all keys, dividing by √dk, and applying softmax.", "contexts": ["att_p8"], "meta": {"category": "scaled_dot_product", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["How are attention weights calculated?", "Explain the attention weight computation"], "synonyms": ["weights", "scores"]}}
{"qid": "att_q21", "question": "What is the residual connection formula in the Transformer?", "answer": "The output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.", "contexts": ["att_p18"], "meta": {"category": "encoder", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q22", "question": "What tokenization method was used?", "answer": "Byte-pair encoding was used, with a shared source-target vocabulary of about 37000 tokens for English-German and 32000 for English-French.", "contexts": ["att_p36"], "meta": {"category": "training", "difficulty": "easy"}, "task_type": "rag_qa", "required": false}
{"qid": "att_q23", "question": "Why was the sine/cosine positional encoding chosen?", "answer": "This function was chosen because it allows the model to easily learn to attend by relative positions, since PE_pos+k can be represented as a linear function of PE_pos.", "contexts": ["att_p24"], "meta": {"category": "positional_encoding", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["simple", "cot", "scaffold"], "paraphrase_runs": 1}}
{"qid": "att_q24", "question": "How does self-attention in the decoder work differently from the encoder?", "answer": "Decoder self-attention allows each position to attend to all positions up to and including that position, while encoder self-attention allows attending to all positions in the previous layer.", "contexts": ["att_p26", "att_p27"], "meta": {"category": "self_attention", "difficulty": "hard"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Compare encoder and decoder self-attention", "What's different about decoder self-attention?"], "require_hybrid": true, "mmr_lambda": 0.3}}
{"qid": "att_q25", "question": "What happens when queries, keys, and values are packed into matrices?", "answer": "When packed into matrices Q, K, and V, the attention function can be computed on a set of queries simultaneously, making the computation more efficient.", "contexts": ["att_p9"], "meta": {"category": "scaled_dot_product", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q26", "question": "What are the projection matrices in multi-head attention?", "answer": "The projection matrices are W^Q_i ∈ R^(dmodel×dk), W^K_i ∈ R^(dmodel×dk), W^V_i ∈ R^(dmodel×dv) and W^O ∈ R^(hdv×dmodel).", "contexts": ["att_p14"], "meta": {"category": "multi_head", "difficulty": "hard"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q27", "question": "Did the authors try learned positional embeddings?", "answer": "Yes, they experimented with learned positional embeddings instead of sine/cosine functions and found that the two versions produced nearly identical results.", "contexts": ["att_p33"], "meta": {"category": "positional_encoding", "difficulty": "medium"}, "task_type": "rag_qa", "required": false}
{"qid": "att_q28", "question": "What is the overall architecture of the Transformer?", "answer": "The Transformer uses stacked self-attention and point-wise, fully connected layers for both the encoder and decoder.", "contexts": ["att_p16"], "meta": {"category": "architecture", "difficulty": "easy"}, "task_type": "rag_qa", "required": true, "robustness": {"paraphrases": ["Describe the Transformer architecture", "How is the Transformer structured?"]}}
{"qid": "att_q29", "question": "What makes attention mechanisms useful for sequence modeling?", "answer": "Attention mechanisms allow modeling of dependencies without regard to their distance in the input or output sequences.", "contexts": ["att_p3"], "meta": {"category": "attention_mechanism", "difficulty": "medium"}, "task_type": "rag_qa", "required": true}
{"qid": "att_q30", "question": "How does the Transformer draw global dependencies?", "answer": "The Transformer relies entirely on an attention mechanism to draw global dependencies between input and output, eschewing recurrence completely.", "contexts": ["att_p1", "att_p4"], "meta": {"category": "architecture", "difficulty": "medium"}, "task_type": "rag_qa", "required": true, "prompt_robustness": {"enabled": true, "modes": ["simple", "cot"], "paraphrase_runs": 2}}
