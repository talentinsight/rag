{"id": "att_p1", "text": "The Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.", "meta": {"source": "attention_paper", "category": "architecture", "section": "abstract", "page": 1}}
{"id": "att_p2", "text": "We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.", "meta": {"source": "attention_paper", "category": "architecture", "section": "abstract", "page": 1}}
{"id": "att_p3", "text": "Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks, allowing modeling of dependencies without regard to their distance in the input or output sequences.", "meta": {"source": "attention_paper", "category": "attention_mechanism", "section": "introduction", "page": 1}}
{"id": "att_p4", "text": "In this work we propose the Transformer, a model architecture eschewing recurrence and instead relying entirely on an attention mechanism to draw global dependencies between input and output.", "meta": {"source": "attention_paper", "category": "architecture", "section": "introduction", "page": 1}}
{"id": "att_p5", "text": "An attention function can be described as mapping a query and a set of key-value pairs to an output, where the query, keys, values, and output are all vectors.", "meta": {"source": "attention_paper", "category": "attention_mechanism", "section": "attention", "page": 3}}
{"id": "att_p6", "text": "The output is computed as a weighted sum of the values, where the weight assigned to each value is computed by a compatibility function of the query with the corresponding key.", "meta": {"source": "attention_paper", "category": "attention_mechanism", "section": "attention", "page": 3}}
{"id": "att_p7", "text": "We call our particular attention Scaled Dot-Product Attention. The input consists of queries and keys of dimension dk, and values of dimension dv.", "meta": {"source": "attention_paper", "category": "scaled_dot_product", "section": "attention", "page": 3}}
{"id": "att_p8", "text": "We compute the dot products of the query with all keys, divide each by √dk, and apply a softmax function to obtain the weights on the values.", "meta": {"source": "attention_paper", "category": "scaled_dot_product", "section": "attention", "page": 3}}
{"id": "att_p9", "text": "In practice, we compute the attention function on a set of queries simultaneously, packed together into a matrix Q. Similarly, the keys and values are packed together into matrices K and V.", "meta": {"source": "attention_paper", "category": "scaled_dot_product", "section": "attention", "page": 3}}
{"id": "att_p10", "text": "Attention(Q, K, V) = softmax(QK^T / √dk)V", "meta": {"source": "attention_paper", "category": "formula", "section": "attention", "page": 3}}
{"id": "att_p11", "text": "Multi-head attention allows the model to jointly attend to information from different representation subspaces at different positions.", "meta": {"source": "attention_paper", "category": "multi_head", "section": "attention", "page": 4}}
{"id": "att_p12", "text": "With a single attention head, averaging inhibits this. Instead of performing a single attention function with dmodel-dimensional keys, values and queries, we found it beneficial to linearly project the queries, keys and values h times with different, learned linear projections to dk, dk and dv dimensions, respectively.", "meta": {"source": "attention_paper", "category": "multi_head", "section": "attention", "page": 4}}
{"id": "att_p13", "text": "MultiHead(Q, K, V) = Concat(head1, ..., headh)W^O where headi = Attention(QW^Q_i, KW^K_i, VW^V_i)", "meta": {"source": "attention_paper", "category": "formula", "section": "attention", "page": 4}}
{"id": "att_p14", "text": "Where the projections are parameter matrices W^Q_i ∈ R^(dmodel×dk), W^K_i ∈ R^(dmodel×dk), W^V_i ∈ R^(dmodel×dv) and W^O ∈ R^(hdv×dmodel).", "meta": {"source": "attention_paper", "category": "multi_head", "section": "attention", "page": 4}}
{"id": "att_p15", "text": "In this work we employ h = 8 parallel attention heads. For each of these we use dk = dv = dmodel/h = 64.", "meta": {"source": "attention_paper", "category": "multi_head", "section": "attention", "page": 4}}
{"id": "att_p16", "text": "The Transformer follows this overall architecture using stacked self-attention and point-wise, fully connected layers for both the encoder and decoder, shown in the left and right halves of Figure 1, respectively.", "meta": {"source": "attention_paper", "category": "architecture", "section": "model_architecture", "page": 3}}
{"id": "att_p17", "text": "Encoder: The encoder is composed of a stack of N = 6 identical layers. Each layer has two sub-layers. The first is a multi-head self-attention mechanism, and the second is a simple, position-wise fully connected feed-forward network.", "meta": {"source": "attention_paper", "category": "encoder", "section": "model_architecture", "page": 3}}
{"id": "att_p18", "text": "We employ a residual connection around each of the two sub-layers, followed by layer normalization. That is, the output of each sub-layer is LayerNorm(x + Sublayer(x)), where Sublayer(x) is the function implemented by the sub-layer itself.", "meta": {"source": "attention_paper", "category": "encoder", "section": "model_architecture", "page": 3}}
{"id": "att_p19", "text": "Decoder: The decoder is also composed of a stack of N = 6 identical layers. In addition to the two sub-layers in each encoder layer, the decoder inserts a third sub-layer, which performs multi-head attention over the output of the encoder stack.", "meta": {"source": "attention_paper", "category": "decoder", "section": "model_architecture", "page": 3}}
{"id": "att_p20", "text": "We also modify the self-attention sub-layer in the decoder stack to prevent positions from attending to subsequent positions. This masking, combined with fact that the output embeddings are offset by one position, ensures that the predictions for position i can depend only on the known outputs at positions less than i.", "meta": {"source": "attention_paper", "category": "decoder", "section": "model_architecture", "page": 3}}
{"id": "att_p21", "text": "Since our model contains no recurrence and no convolution, in order for the model to make use of the order of the sequence, we must inject some information about the relative or absolute position of the tokens in the sequence.", "meta": {"source": "attention_paper", "category": "positional_encoding", "section": "model_architecture", "page": 5}}
{"id": "att_p22", "text": "To this end, we add positional encodings to the input embeddings at the bottoms of the encoder and decoder stacks. The positional encodings have the same dimension dmodel as the embeddings, so that the two can be summed.", "meta": {"source": "attention_paper", "category": "positional_encoding", "section": "model_architecture", "page": 5}}
{"id": "att_p23", "text": "In this work, we use sine and cosine functions of different frequencies: PE(pos,2i) = sin(pos/10000^(2i/dmodel)) PE(pos,2i+1) = cos(pos/10000^(2i/dmodel))", "meta": {"source": "attention_paper", "category": "positional_encoding", "section": "model_architecture", "page": 5}}
{"id": "att_p24", "text": "We chose this function because we hypothesized it would allow the model to easily learn to attend by relative positions, since for any fixed offset k, PE_pos+k can be represented as a linear function of PE_pos.", "meta": {"source": "attention_paper", "category": "positional_encoding", "section": "model_architecture", "page": 5}}
{"id": "att_p25", "text": "In the encoder self-attention layers, all of the keys, values and queries come from the same place, in this case, the output of the previous layer in the encoder.", "meta": {"source": "attention_paper", "category": "self_attention", "section": "attention_applications", "page": 4}}
{"id": "att_p26", "text": "Each position in the encoder can attend to all positions in the previous layer of the encoder.", "meta": {"source": "attention_paper", "category": "self_attention", "section": "attention_applications", "page": 4}}
{"id": "att_p27", "text": "Similarly, self-attention layers in the decoder allow each position in the decoder to attend to all positions in the decoder up to and including that position.", "meta": {"source": "attention_paper", "category": "self_attention", "section": "attention_applications", "page": 4}}
{"id": "att_p28", "text": "We need to prevent leftward information flow in the decoder to preserve the auto-regressive property. We implement this inside of scaled dot-product attention by masking out (setting to −∞) all values in the input of the softmax which correspond to illegal connections.", "meta": {"source": "attention_paper", "category": "masking", "section": "attention_applications", "page": 4}}
{"id": "att_p29", "text": "The encoder-decoder attention layers, the queries come from the previous decoder layer, and the memory keys and values come from the output of the encoder.", "meta": {"source": "attention_paper", "category": "encoder_decoder_attention", "section": "attention_applications", "page": 4}}
{"id": "att_p30", "text": "This allows every position in the decoder to attend over all positions in the input sequence. This mimics the typical encoder-decoder attention mechanisms in sequence-to-sequence models.", "meta": {"source": "attention_paper", "category": "encoder_decoder_attention", "section": "attention_applications", "page": 4}}
{"id": "att_p31", "text": "Self-attention could yield more interpretable models. We inspect attention distributions from our models and present and discuss examples in the appendix.", "meta": {"source": "attention_paper", "category": "interpretability", "section": "why_self_attention", "page": 5}}
{"id": "att_p32", "text": "Individual attention heads clearly learn to perform different tasks. Many appear to exhibit behavior related to the syntactic and semantic structure of the sentences.", "meta": {"source": "attention_paper", "category": "interpretability", "section": "why_self_attention", "page": 5}}
{"id": "att_p33", "text": "As side information, we also experimented with using learned positional embeddings instead, and found that the two versions produced nearly identical results.", "meta": {"source": "attention_paper", "category": "positional_encoding", "section": "model_architecture", "page": 5}}
{"id": "att_p34", "text": "We trained on the standard WMT 2014 English-German dataset consisting of about 4.5 million sentence pairs.", "meta": {"source": "attention_paper", "category": "training", "section": "experiments", "page": 6}}
{"id": "att_p35", "text": "For the WMT 2014 English-French translation task, we used the significantly larger WMT 2014 English-French dataset consisting of 36M sentences.", "meta": {"source": "attention_paper", "category": "training", "section": "experiments", "page": 6}}
{"id": "att_p36", "text": "We used byte-pair encoding, which has a shared source-target vocabulary of about 37000 tokens for English-German and 32000 for English-French.", "meta": {"source": "attention_paper", "category": "training", "section": "experiments", "page": 6}}
{"id": "att_p37", "text": "Our model achieves 28.4 BLEU on the WMT 2014 English-German translation task, improving over the existing best results, including ensembles, by over 2 BLEU.", "meta": {"source": "attention_paper", "category": "results", "section": "experiments", "page": 6}}
{"id": "att_p38", "text": "On the WMT 2014 English-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.0, improving over the existing best results by over 2.0 BLEU.", "meta": {"source": "attention_paper", "category": "results", "section": "experiments", "page": 6}}
{"id": "att_p39", "text": "Training took 3.5 days on 8 P100 GPUs for the base models and 12 days for the big models.", "meta": {"source": "attention_paper", "category": "training", "section": "experiments", "page": 7}}
{"id": "att_p40", "text": "The big model consists of N = 6, dmodel = 1024, dff = 4096, h = 16, dk = dv = 64, dropout = 0.3.", "meta": {"source": "attention_paper", "category": "model_details", "section": "experiments", "page": 7}}
